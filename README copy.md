# AI Hallucination and Citation Verification System

A comprehensive system that detects, flags, and verifies factual claims and citations generated by AI models, helping users distinguish between reliable and unreliable AI-produced information.

## üéØ Features

- **Claim Extraction**: Automatically extracts atomic factual claims from AI-generated text
- **Citation Verification**: Verifies if citations exist in academic databases (CrossRef, Semantic Scholar)
- **Fact Verification**: Validates claims against trusted sources using retrieval and NLI models
- **Hallucination Scoring**: Computes risk scores based on unverified claims, fake citations, and contradictions
- **User-Friendly Reports**: Generates detailed reports with specific issues and recommendations

## üèóÔ∏è Architecture

```
AI Generated Text
        ‚Üì
Claim & Citation Extractor
        ‚Üì
Verification Engine
   ‚îú‚îÄ‚îÄ Citation Verification (CrossRef, Semantic Scholar)
   ‚îú‚îÄ‚îÄ Fact Verification (Wikipedia, NLI models)
        ‚Üì
Hallucination Scorer
        ‚Üì
User-Friendly Report
```

## üìã Requirements

- Python 3.8+
- pip

## üöÄ Installation

1. **Clone or navigate to the project directory**

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Run setup script** (downloads NLTK data and verifies installation):
```bash
python setup.py
```

   Or manually download NLTK data:
```python
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
```

4. **Optional: Set up API keys** (create a `.env` file):
```env
CROSSREF_API_KEY=your_key_here
SEMANTIC_SCHOLAR_API_KEY=your_key_here
```

Note: The system works without API keys, but some features may be limited. API keys can be obtained from:
- CrossRef: https://www.crossref.org/services/metadata-delivery/rest-api/
- Semantic Scholar: https://www.semanticscholar.org/product/api

## üéÆ Usage

### Option 1: Streamlit Web Interface (Recommended)

1. **Start the API Server** (in one terminal):
```bash
python main.py
```

2. **Start the Streamlit App** (in another terminal):
```bash
streamlit run app.py
```

3. The Streamlit interface will open automatically in your browser at `http://localhost:8501`

4. Paste your AI-generated text and click "Verify Text"

### Option 2: API Only

Start just the API server:
```bash
python main.py
```

The API will be available at `http://localhost:8000`

### Use the API Directly

```bash
curl -X POST "http://localhost:8000/verify" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "According to Smith et al. (2021), GPT models reduce hallucinations by 73%.",
    "verify_citations": true,
    "verify_facts": true
  }'
```

### Python Example

```python
import requests

response = requests.post(
    "http://localhost:8000/verify",
    json={
        "text": "Your AI-generated text here...",
        "verify_citations": True,
        "verify_facts": True
    }
)

result = response.json()
print(f"Risk Level: {result['overall_risk']}")
print(f"Risk Score: {result['risk_score']}")
print(f"Issues Found: {len(result['issues'])}")
```

## üìä Response Format

```json
{
  "overall_risk": "high|medium|low",
  "risk_score": 75.5,
  "total_claims": 5,
  "total_citations": 3,
  "verified_claims": 2,
  "fake_citations": 1,
  "unverified_claims": 2,
  "contradicted_claims": 1,
  "broken_links": 0,
  "issues": [
    {
      "type": "fake_citation",
      "severity": "high",
      "detail": "Citation 'Smith et al. (2021)' not found...",
      "location": {"start": 0, "end": 25},
      "recommendation": "Verify the citation manually..."
    }
  ],
  "detailed_results": {
    "citation_verifications": [...],
    "fact_verifications": [...]
  }
}
```

## üîß Configuration

Edit `config.py` to customize:

- **Scoring weights**: Adjust how different factors contribute to risk score
- **Risk thresholds**: Change what constitutes low/medium/high risk
- **Models**: Switch NLI or embedding models
- **API endpoints**: Configure external service URLs

## üß™ Example Use Cases

1. **Research Paper Review**: Verify citations in AI-generated literature reviews
2. **Educational Content**: Check factual accuracy of AI-generated study materials
3. **Legal Documents**: Flag unverified claims in AI-generated legal summaries
4. **News Articles**: Detect hallucinations in AI-written news content

## üõ†Ô∏è Components

- **`extractors.py`**: Claim and citation extraction using NLP
- **`citation_verifier.py`**: Citation verification via academic databases
- **`fact_verifier.py`**: Fact verification using retrieval and NLI
- **`scorer.py`**: Hallucination risk scoring and report generation
- **`main.py`**: FastAPI application and endpoints
- **`app.py`**: Streamlit web interface
- **`config.py`**: Configuration settings

## ‚ö†Ô∏è Limitations

- **Rate Limits**: Academic APIs have rate limits; batch processing includes delays
- **Model Accuracy**: NLI models may have false positives/negatives
- **Coverage**: Not all citation formats are supported (focuses on APA, MLA, IEEE, URLs, DOIs)
- **Language**: Currently optimized for English text

## üîÆ Future Enhancements

- Support for more citation formats
- Integration with more academic databases
- Real-time verification during text generation
- Multi-language support
- Browser extension for web content verification
- Machine learning model fine-tuning on hallucination patterns

## üìù License

This project is provided as-is for educational and research purposes.

## ü§ù Contributing

Contributions welcome! Areas for improvement:
- Additional citation format support
- More fact-checking sources
- Performance optimizations
- UI/UX improvements

## üìß Support

For issues or questions, please open an issue in the repository.

---

**Built with ‚ù§Ô∏è for trustworthy AI**

